{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c6e273",
   "metadata": {},
   "source": [
    "merge all dataset into one data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6912b984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded crude_oil_wti_cleaned_20250702_114456.csv\n",
      "Loaded data_inflasi_cleaned_20250702_102841.csv\n",
      "Loaded federal_funds_rate_cleaned_20250630_195749.csv\n",
      "Loaded harga_emas_cleaned.csv\n",
      "Loaded jakarta_stock_exchange_cleaned_20250702_095758.csv\n",
      "Final merged dataset shape: (34, 21)\n",
      "Columns: ['tanggal', 'WTI_Price_USD', 'inflation_rate', 'close_rate', 'open_rate', 'high_rate', 'low_rate', 'volume_billion_x', 'change_percent_x', 'usd_per_ounce', 'kurs_dollar_per_ounce', 'idr_per_ounce', 'usd_per_gram', 'kurs_dollar_per_gram', 'idr_per_gram', 'close_index', 'open_index', 'high_index', 'low_index', 'volume_billion_y', 'change_percent_y']\n",
      "Date range: 2020-04-01 00:00:00 to 2024-11-01 00:00:00\n",
      "\n",
      "‚úÖ Final merged dataset saved to: final_merged_dataset_20250703_110915.csv\n",
      "Full path: D:\\College Life\\Semester 4\\Big Data & Predictive Analysis\\Final Project\\Testing\\final_merged_dataset_20250703_110915.csv\n",
      "\n",
      "First 5 rows of merged dataset:\n",
      "     tanggal  WTI_Price_USD  inflation_rate  close_rate  open_rate  high_rate  \\\n",
      "0 2020-04-01          20.28            2.67        0.06       0.06       0.06   \n",
      "1 2020-07-01          39.88            1.54        0.08       0.08       0.08   \n",
      "2 2020-09-01          42.76            1.42        0.09       0.09       0.09   \n",
      "3 2020-10-01          38.51            1.44        0.09       0.09       0.09   \n",
      "4 2020-12-01          44.54            1.68        0.09       0.09       0.09   \n",
      "\n",
      "   low_rate  volume_billion_x  change_percent_x  usd_per_ounce  ...  \\\n",
      "0      0.06               NaN             -25.0        1584.40  ...   \n",
      "1      0.08               NaN               0.0        1769.95  ...   \n",
      "2      0.09               NaN               0.0        1976.08  ...   \n",
      "3      0.09               NaN               0.0        1908.54  ...   \n",
      "4      0.09               NaN               0.0        1809.96  ...   \n",
      "\n",
      "   idr_per_ounce  usd_per_gram  kurs_dollar_per_gram  idr_per_gram  \\\n",
      "0       26004773         50.94              16413.01        836073   \n",
      "1       25382871         56.91              14341.01        816078   \n",
      "2       28880429         63.53              14615.01        928527   \n",
      "3       28391441         61.36              14876.00        912806   \n",
      "4       25661613         58.19              14178.00        825040   \n",
      "\n",
      "   close_index  open_index  high_index  low_index  volume_billion_y  \\\n",
      "0      4466.04     4538.93     4627.42    4445.14      4.180000e+09   \n",
      "1      4914.39     4905.39     4928.61    4885.60      4.320000e+09   \n",
      "2      5310.68     5238.49     5310.68    5219.21      9.050000e+09   \n",
      "3      4970.09     4899.64     4970.09    4899.00      9.640000e+09   \n",
      "4      5724.74     5637.89     5736.32    5594.28      2.095000e+10   \n",
      "\n",
      "   change_percent_y  \n",
      "0             -1.61  \n",
      "1              0.18  \n",
      "2              1.38  \n",
      "3              2.05  \n",
      "4              2.00  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Dataset Info:\n",
      "- Total rows: 34\n",
      "- Total columns: 21\n",
      "- Memory usage: 0.01 MB\n"
     ]
    }
   ],
   "source": [
    "# If you want to merge specific files by \n",
    "\n",
    "import pandas as pd;\n",
    "import os as os;\n",
    "from functools import reduce  # Add this import\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "specific_files = ['crude_oil_wti_cleaned_20250702_114456.csv', 'data_inflasi_cleaned_20250702_102841.csv', 'federal_funds_rate_cleaned_20250630_195749.csv','harga_emas_cleaned.csv','jakarta_stock_exchange_cleaned_20250702_095758.csv']  # Replace with your actual file names\n",
    "data_path = r\"D:\\College Life\\Semester 4\\Big Data & Predictive Analysis\\Final Project\\Testing\"\n",
    "\n",
    "dfs = []\n",
    "for file in specific_files:\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'tanggal' in df.columns:\n",
    "            df['tanggal'] = pd.to_datetime(df['tanggal'])\n",
    "        dfs.append(df)\n",
    "        print(f\"Loaded {file}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file}\")\n",
    "\n",
    "# Merge using the reduce method\n",
    "if len(dfs) > 1:\n",
    "    final_merged = reduce(lambda left, right: pd.merge(left, right, on='tanggal', how='inner'), dfs)\n",
    "    print(f\"Final merged dataset shape: {final_merged.shape}\")\n",
    "    print(f\"Columns: {list(final_merged.columns)}\")\n",
    "    print(f\"Date range: {final_merged['tanggal'].min()} to {final_merged['tanggal'].max()}\")\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"final_merged_dataset_{timestamp}.csv\"\n",
    "    output_path = os.path.join(data_path, output_filename)\n",
    "    \n",
    "    # Save to new file\n",
    "    final_merged.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ Final merged dataset saved to: {output_filename}\")\n",
    "    print(f\"Full path: {output_path}\")\n",
    "    \n",
    "    # Display sample of merged data\n",
    "    print(\"\\nFirst 5 rows of merged dataset:\")\n",
    "    print(final_merged.head())\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(f\"\\nDataset Info:\")\n",
    "    print(f\"- Total rows: {len(final_merged)}\")\n",
    "    print(f\"- Total columns: {len(final_merged.columns)}\")\n",
    "    print(f\"- Memory usage: {final_merged.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Need at least 2 dataframes to merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb440312",
   "metadata": {},
   "source": [
    "# Merge Dataset dengan Data Emas sebagai Struktur Utama\n",
    "Pendekatan ini menggunakan data emas sebagai base dataset dan melakukan left join dengan dataset lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf2eb915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded gold data: (1899, 7)\n",
      "Gold data columns: ['tanggal', 'usd_per_ounce', 'kurs_dollar_per_ounce', 'idr_per_ounce', 'usd_per_gram', 'kurs_dollar_per_gram', 'idr_per_gram']\n",
      "Gold date range: 2020-01-01 00:00:00 to 2025-03-13 00:00:00\n",
      "‚úÖ Merged with crude_oil_wti_cleaned_20250702_114456.csv: Shape (1899, 8)\n",
      "‚úÖ Merged with data_inflasi_cleaned_20250702_102841.csv: Shape (1899, 9)\n",
      "‚úÖ Merged with federal_funds_rate_cleaned_20250630_195749.csv: Shape (1899, 15)\n",
      "‚úÖ Merged with jakarta_stock_exchange_cleaned_20250702_095758.csv: Shape (1899, 21)\n",
      "\n",
      "üìä Final merged dataset (Gold-based):\n",
      "- Shape: (1899, 21)\n",
      "- Columns: ['tanggal', 'usd_per_ounce', 'kurs_dollar_per_ounce', 'idr_per_ounce', 'usd_per_gram', 'kurs_dollar_per_gram', 'idr_per_gram', 'WTI_Price_USD', 'inflation_rate', 'close_rate', 'open_rate', 'high_rate', 'low_rate', 'volume_billion_x', 'change_percent_x', 'close_index', 'open_index', 'high_index', 'low_index', 'volume_billion_y', 'change_percent_y']\n",
      "- Date range: 2020-01-01 00:00:00 to 2025-03-13 00:00:00\n",
      "- Missing values per column:\n",
      "tanggal                     0\n",
      "usd_per_ounce               0\n",
      "kurs_dollar_per_ounce       0\n",
      "idr_per_ounce               0\n",
      "usd_per_gram                0\n",
      "kurs_dollar_per_gram        0\n",
      "idr_per_gram                0\n",
      "WTI_Price_USD             599\n",
      "inflation_rate           1836\n",
      "close_rate                599\n",
      "open_rate                 599\n",
      "high_rate                 599\n",
      "low_rate                  599\n",
      "volume_billion_x         1899\n",
      "change_percent_x          599\n",
      "close_index               638\n",
      "open_index                638\n",
      "high_index                638\n",
      "low_index                 638\n",
      "volume_billion_y          641\n",
      "change_percent_y          638\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ Gold-based merged dataset saved to: gold_based_merged_dataset_20250703_111551.csv\n",
      "Full path: D:\\College Life\\Semester 4\\Big Data & Predictive Analysis\\Final Project\\Testing\\gold_based_merged_dataset_20250703_111551.csv\n",
      "\n",
      "First 10 rows of gold-based merged dataset:\n",
      "     tanggal  usd_per_ounce  kurs_dollar_per_ounce  idr_per_ounce  \\\n",
      "0 2020-01-01        1517.24               13901.01       21091168   \n",
      "1 2020-01-02        1526.12               13895.01       21205453   \n",
      "2 2020-01-03        1548.99               13899.01       21529427   \n",
      "3 2020-01-04        1552.24               13899.01       21574599   \n",
      "4 2020-01-05        1552.24               13899.01       21574599   \n",
      "5 2020-01-06        1564.34               13961.01       21839766   \n",
      "6 2020-01-07        1571.29               13919.01       21870801   \n",
      "7 2020-01-08        1555.80               13934.00       21678517   \n",
      "8 2020-01-09        1549.66               13860.00       21478288   \n",
      "9 2020-01-10        1559.73               13812.00       21542991   \n",
      "\n",
      "   usd_per_gram  kurs_dollar_per_gram  idr_per_gram  WTI_Price_USD  \\\n",
      "0         48.78              13901.01        678097            NaN   \n",
      "1         49.07              13895.01        681771          61.17   \n",
      "2         49.80              13899.01        692187          63.00   \n",
      "3         49.91              13899.01        693639            NaN   \n",
      "4         49.91              13899.01        693639            NaN   \n",
      "5         50.29              13961.01        702165          63.27   \n",
      "6         50.52              13919.01        703163          62.70   \n",
      "7         50.02              13934.00        696981          59.65   \n",
      "8         49.82              13860.00        690543          59.56   \n",
      "9         50.15              13812.00        692623          59.02   \n",
      "\n",
      "   inflation_rate  close_rate  ...  high_rate  low_rate  volume_billion_x  \\\n",
      "0            2.68         NaN  ...        NaN       NaN               NaN   \n",
      "1             NaN        1.55  ...       1.55      1.55               NaN   \n",
      "2             NaN        1.55  ...       1.55      1.55               NaN   \n",
      "3             NaN         NaN  ...        NaN       NaN               NaN   \n",
      "4             NaN         NaN  ...        NaN       NaN               NaN   \n",
      "5             NaN        1.55  ...       1.55      1.55               NaN   \n",
      "6             NaN        1.55  ...       1.55      1.55               NaN   \n",
      "7             NaN        1.55  ...       1.55      1.55               NaN   \n",
      "8             NaN        1.55  ...       1.55      1.55               NaN   \n",
      "9             NaN        1.54  ...       1.54      1.54               NaN   \n",
      "\n",
      "   change_percent_x  close_index  open_index  high_index  low_index  \\\n",
      "0               NaN          NaN         NaN         NaN        NaN   \n",
      "1              0.00      6283.58     6313.13     6317.01    6263.68   \n",
      "2              0.00      6323.47     6306.19     6323.47    6287.71   \n",
      "3               NaN          NaN         NaN         NaN        NaN   \n",
      "4               NaN          NaN         NaN         NaN        NaN   \n",
      "5              0.00      6257.40     6293.50     6300.44    6252.64   \n",
      "6              0.00      6279.35     6272.22     6284.89    6246.13   \n",
      "7              0.00      6225.69     6248.44     6250.12    6218.13   \n",
      "8              0.00      6274.49     6248.66     6274.49    6238.98   \n",
      "9             -0.65      6274.94     6287.17     6295.37    6271.98   \n",
      "\n",
      "   volume_billion_y  change_percent_y  \n",
      "0               NaN               NaN  \n",
      "1      3.300000e+09             -0.25  \n",
      "2      4.460000e+09              0.63  \n",
      "3               NaN               NaN  \n",
      "4               NaN               NaN  \n",
      "5      4.140000e+09             -1.04  \n",
      "6      3.940000e+09              0.35  \n",
      "7      4.460000e+09             -0.85  \n",
      "8      4.190000e+09              0.78  \n",
      "9      5.220000e+09              0.01  \n",
      "\n",
      "[10 rows x 21 columns]\n",
      "\n",
      "Dataset Statistics:\n",
      "- Total rows: 1899\n",
      "- Total columns: 21\n",
      "- Data completeness: 72.02%\n"
     ]
    }
   ],
   "source": [
    "# Merge dengan Data Emas sebagai Struktur Utama\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Path ke folder data\n",
    "data_path = r\"D:\\College Life\\Semester 4\\Big Data & Predictive Analysis\\Final Project\\Testing\"\n",
    "\n",
    "# Load data emas sebagai base dataset\n",
    "gold_file = 'harga_emas_cleaned.csv'\n",
    "gold_path = os.path.join(data_path, gold_file)\n",
    "\n",
    "if os.path.exists(gold_path):\n",
    "    # Load data emas sebagai base\n",
    "    gold_df = pd.read_csv(gold_path)\n",
    "    if 'tanggal' in gold_df.columns:\n",
    "        gold_df['tanggal'] = pd.to_datetime(gold_df['tanggal'])\n",
    "    \n",
    "    print(f\"‚úÖ Loaded gold data: {gold_df.shape}\")\n",
    "    print(f\"Gold data columns: {list(gold_df.columns)}\")\n",
    "    print(f\"Gold date range: {gold_df['tanggal'].min()} to {gold_df['tanggal'].max()}\")\n",
    "    \n",
    "    # Load dataset lainnya\n",
    "    other_files = [\n",
    "        'crude_oil_wti_cleaned_20250702_114456.csv',\n",
    "        'data_inflasi_cleaned_20250702_102841.csv', \n",
    "        'federal_funds_rate_cleaned_20250630_195749.csv',\n",
    "        'jakarta_stock_exchange_cleaned_20250702_095758.csv'\n",
    "    ]\n",
    "    \n",
    "    # Mulai dengan data emas sebagai base\n",
    "    merged_gold_base = gold_df.copy()\n",
    "    \n",
    "    # Merge dataset lainnya menggunakan left join\n",
    "    for file in other_files:\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            if 'tanggal' in df.columns:\n",
    "                df['tanggal'] = pd.to_datetime(df['tanggal'])\n",
    "            \n",
    "            # Left join untuk mempertahankan semua data emas\n",
    "            merged_gold_base = pd.merge(merged_gold_base, df, on='tanggal', how='left')\n",
    "            print(f\"‚úÖ Merged with {file}: Shape {merged_gold_base.shape}\")\n",
    "        else:\n",
    "            print(f\"‚ùå File not found: {file}\")\n",
    "    \n",
    "    # Informasi dataset hasil merge\n",
    "    print(f\"\\nüìä Final merged dataset (Gold-based):\")\n",
    "    print(f\"- Shape: {merged_gold_base.shape}\")\n",
    "    print(f\"- Columns: {list(merged_gold_base.columns)}\")\n",
    "    print(f\"- Date range: {merged_gold_base['tanggal'].min()} to {merged_gold_base['tanggal'].max()}\")\n",
    "    print(f\"- Missing values per column:\")\n",
    "    print(merged_gold_base.isnull().sum())\n",
    "    \n",
    "    # Generate filename dengan timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"gold_based_merged_dataset_{timestamp}.csv\"\n",
    "    output_path = os.path.join(data_path, output_filename)\n",
    "    \n",
    "    # Save dataset\n",
    "    merged_gold_base.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ Gold-based merged dataset saved to: {output_filename}\")\n",
    "    print(f\"Full path: {output_path}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\nFirst 10 rows of gold-based merged dataset:\")\n",
    "    print(merged_gold_base.head(10))\n",
    "    \n",
    "    # Statistik dasar\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"- Total rows: {len(merged_gold_base)}\")\n",
    "    print(f\"- Total columns: {len(merged_gold_base.columns)}\")\n",
    "    print(f\"- Data completeness: {(1 - merged_gold_base.isnull().sum().sum() / (merged_gold_base.shape[0] * merged_gold_base.shape[1])) * 100:.2f}%\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Gold file not found: {gold_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58dc88c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Checking individual file row counts:\n",
      "- harga_emas_cleaned.csv: 1899 rows\n",
      "  Date range: 2020-01-01 00:00:00 to 2025-03-13 00:00:00\n",
      "- crude_oil_wti_cleaned_20250702_114456.csv: 1301 rows\n",
      "  Date range: 2019-12-31 00:00:00 to 2025-03-13 00:00:00\n",
      "- data_inflasi_cleaned_20250702_102841.csv: 63 rows\n",
      "  Date range: 2020-01-01 00:00:00 to 2025-03-01 00:00:00\n",
      "- federal_funds_rate_cleaned_20250630_195749.csv: 1302 rows\n",
      "  Date range: 2019-12-31 00:00:00 to 2025-03-18 00:00:00\n",
      "- jakarta_stock_exchange_cleaned_20250702_095758.csv: 1262 rows\n",
      "  Date range: 2019-12-30 00:00:00 to 2025-03-13 00:00:00\n",
      "\n",
      "==================================================\n",
      "\n",
      "üìã Latest merged file: gold_based_merged_dataset_20250703_111551.csv\n",
      "- Total rows: 1899\n",
      "- Total columns: 21\n",
      "- Date range: 2020-01-01 00:00:00 to 2025-03-13 00:00:00\n",
      "\n",
      "üîç Missing values analysis:\n",
      "- WTI_Price_USD: 599 missing values (31.5%)\n",
      "- inflation_rate: 1836 missing values (96.7%)\n",
      "- close_rate: 599 missing values (31.5%)\n",
      "- open_rate: 599 missing values (31.5%)\n",
      "- high_rate: 599 missing values (31.5%)\n",
      "- low_rate: 599 missing values (31.5%)\n",
      "- volume_billion_x: 1899 missing values (100.0%)\n",
      "- change_percent_x: 599 missing values (31.5%)\n",
      "- close_index: 638 missing values (33.6%)\n",
      "- open_index: 638 missing values (33.6%)\n",
      "- high_index: 638 missing values (33.6%)\n",
      "- low_index: 638 missing values (33.6%)\n",
      "- volume_billion_y: 641 missing values (33.8%)\n",
      "- change_percent_y: 638 missing values (33.6%)\n"
     ]
    }
   ],
   "source": [
    "# Verifikasi Jumlah Rows dan Struktur Data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_path = r\"D:\\College Life\\Semester 4\\Big Data & Predictive Analysis\\Final Project\\Testing\"\n",
    "\n",
    "# Cek jumlah rows di setiap file\n",
    "print(\"üìä Checking individual file row counts:\")\n",
    "files_to_check = [\n",
    "    'harga_emas_cleaned.csv',\n",
    "    'crude_oil_wti_cleaned_20250702_114456.csv',\n",
    "    'data_inflasi_cleaned_20250702_102841.csv', \n",
    "    'federal_funds_rate_cleaned_20250630_195749.csv',\n",
    "    'jakarta_stock_exchange_cleaned_20250702_095758.csv'\n",
    "]\n",
    "\n",
    "for file in files_to_check:\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"- {file}: {len(df)} rows\")\n",
    "        if 'tanggal' in df.columns:\n",
    "            df['tanggal'] = pd.to_datetime(df['tanggal'])\n",
    "            print(f\"  Date range: {df['tanggal'].min()} to {df['tanggal'].max()}\")\n",
    "    else:\n",
    "        print(f\"- {file}: File not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Cek hasil merge yang sudah ada\n",
    "latest_files = [f for f in os.listdir(data_path) if f.startswith('gold_based_merged_dataset_')]\n",
    "if latest_files:\n",
    "    latest_file = sorted(latest_files)[-1]  # Get the latest file\n",
    "    latest_path = os.path.join(data_path, latest_file)\n",
    "    merged_df = pd.read_csv(latest_path)\n",
    "    print(f\"\\nüìã Latest merged file: {latest_file}\")\n",
    "    print(f\"- Total rows: {len(merged_df)}\")\n",
    "    print(f\"- Total columns: {len(merged_df.columns)}\")\n",
    "    \n",
    "    if 'tanggal' in merged_df.columns:\n",
    "        merged_df['tanggal'] = pd.to_datetime(merged_df['tanggal'])\n",
    "        print(f\"- Date range: {merged_df['tanggal'].min()} to {merged_df['tanggal'].max()}\")\n",
    "    \n",
    "    print(f\"\\nüîç Missing values analysis:\")\n",
    "    missing_counts = merged_df.isnull().sum()\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"- {col}: {count} missing values ({count/len(merged_df)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No merged files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1f220b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü•á Data Emas (Base Dataset):\n",
      "Total rows: 1899\n",
      "Columns: ['tanggal', 'usd_per_ounce', 'kurs_dollar_per_ounce', 'idr_per_ounce', 'usd_per_gram', 'kurs_dollar_per_gram', 'idr_per_gram']\n",
      "Date range: 2020-01-01 00:00:00 to 2025-03-13 00:00:00\n",
      "First 3 dates: [Timestamp('2020-01-01 00:00:00'), Timestamp('2020-01-02 00:00:00'), Timestamp('2020-01-03 00:00:00')]\n",
      "Last 3 dates: [Timestamp('2025-03-11 00:00:00'), Timestamp('2025-03-12 00:00:00'), Timestamp('2025-03-13 00:00:00')]\n",
      "\n",
      "Expected final rows after merge: 1899 (using left join with gold as base)\n"
     ]
    }
   ],
   "source": [
    "# Cek Data Emas (Base Dataset)\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_path = r\"D:\\College Life\\Semester 4\\Big Data & Predictive Analysis\\Final Project\\Testing\"\n",
    "gold_file = 'harga_emas_cleaned.csv'\n",
    "gold_path = os.path.join(data_path, gold_file)\n",
    "\n",
    "# Load dan cek data emas\n",
    "gold_df = pd.read_csv(gold_path)\n",
    "print(f\"ü•á Data Emas (Base Dataset):\")\n",
    "print(f\"Total rows: {len(gold_df)}\")\n",
    "print(f\"Columns: {list(gold_df.columns)}\")\n",
    "\n",
    "if 'tanggal' in gold_df.columns:\n",
    "    gold_df['tanggal'] = pd.to_datetime(gold_df['tanggal'])\n",
    "    print(f\"Date range: {gold_df['tanggal'].min()} to {gold_df['tanggal'].max()}\")\n",
    "    print(f\"First 3 dates: {gold_df['tanggal'].head(3).tolist()}\")\n",
    "    print(f\"Last 3 dates: {gold_df['tanggal'].tail(3).tolist()}\")\n",
    "\n",
    "print(f\"\\nExpected final rows after merge: {len(gold_df)} (using left join with gold as base)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c88c9b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Hasil Merge Gold-Based: gold_based_merged_dataset_20250703_111551.csv\n",
      "Total rows: 1899\n",
      "Total columns: 21\n",
      "‚úÖ BENAR! Jumlah rows sesuai dengan data emas (1899)\n",
      "\n",
      "Missing values total: 11160\n",
      "Kolom dengan missing values:\n",
      "- WTI_Price_USD: 599 missing (31.5%)\n",
      "- inflation_rate: 1836 missing (96.7%)\n",
      "- close_rate: 599 missing (31.5%)\n",
      "- open_rate: 599 missing (31.5%)\n",
      "- high_rate: 599 missing (31.5%)\n",
      "- low_rate: 599 missing (31.5%)\n",
      "- volume_billion_x: 1899 missing (100.0%)\n",
      "- change_percent_x: 599 missing (31.5%)\n",
      "- close_index: 638 missing (33.6%)\n",
      "- open_index: 638 missing (33.6%)\n",
      "- high_index: 638 missing (33.6%)\n",
      "- low_index: 638 missing (33.6%)\n",
      "- volume_billion_y: 641 missing (33.8%)\n",
      "- change_percent_y: 638 missing (33.6%)\n"
     ]
    }
   ],
   "source": [
    "# Cek Hasil Merge Gold-Based\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Cari file hasil merge terbaru\n",
    "data_path = r\"D:\\College Life\\Semester 4\\Big Data & Predictive Analysis\\Final Project\\Testing\"\n",
    "gold_based_files = [f for f in os.listdir(data_path) if f.startswith('gold_based_merged_dataset_')]\n",
    "\n",
    "if gold_based_files:\n",
    "    latest_file = sorted(gold_based_files)[-1]\n",
    "    file_path = os.path.join(data_path, latest_file)\n",
    "    \n",
    "    merged_df = pd.read_csv(file_path)\n",
    "    print(f\"üìä Hasil Merge Gold-Based: {latest_file}\")\n",
    "    print(f\"Total rows: {len(merged_df)}\")\n",
    "    print(f\"Total columns: {len(merged_df.columns)}\")\n",
    "    \n",
    "    if len(merged_df) == 1899:\n",
    "        print(\"‚úÖ BENAR! Jumlah rows sesuai dengan data emas (1899)\")\n",
    "    else:\n",
    "        print(f\"‚ùå SALAH! Seharusnya 1899 rows, tapi dapat {len(merged_df)} rows\")\n",
    "    \n",
    "    # Cek missing values\n",
    "    missing_summary = merged_df.isnull().sum()\n",
    "    total_missing = missing_summary.sum()\n",
    "    print(f\"\\nMissing values total: {total_missing}\")\n",
    "    \n",
    "    # Tampilkan kolom dengan missing values\n",
    "    if total_missing > 0:\n",
    "        print(\"Kolom dengan missing values:\")\n",
    "        for col, count in missing_summary.items():\n",
    "            if count > 0:\n",
    "                print(f\"- {col}: {count} missing ({count/len(merged_df)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"‚ùå File gold-based merge tidak ditemukan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb8c3a2",
   "metadata": {},
   "source": [
    "# Fill Missing Values\n",
    "Mengisi missing values pada dataset final dengan berbagai metode:\n",
    "- Forward Fill: mengisi dengan nilai sebelumnya\n",
    "- Backward Fill: mengisi dengan nilai berikutnya  \n",
    "- Interpolasi: mengisi dengan nilai rata-rata antara data sebelum dan sesudah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8137f2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Original dataset: (1899, 21)\n",
      "Missing values before filling:\n",
      "- WTI_Price_USD: 599 missing (31.5%)\n",
      "- inflation_rate: 1836 missing (96.7%)\n",
      "- close_rate: 599 missing (31.5%)\n",
      "- open_rate: 599 missing (31.5%)\n",
      "- high_rate: 599 missing (31.5%)\n",
      "- low_rate: 599 missing (31.5%)\n",
      "- volume_billion_x: 1899 missing (100.0%)\n",
      "- change_percent_x: 599 missing (31.5%)\n",
      "- close_index: 638 missing (33.6%)\n",
      "- open_index: 638 missing (33.6%)\n",
      "- high_index: 638 missing (33.6%)\n",
      "- low_index: 638 missing (33.6%)\n",
      "- volume_billion_y: 641 missing (33.8%)\n",
      "- change_percent_y: 638 missing (33.6%)\n",
      "\n",
      "üîÑ Filling financial data with forward fill...\n",
      "üìà Interpolating inflation rate...\n",
      "üìä Interpolating change_percent_x...\n",
      "üìä Interpolating change_percent_y...\n",
      "üî¢ Filling volume_billion_x with 0 (all missing)...\n",
      "üìä Interpolating volume_billion_y...\n",
      "\n",
      "‚úÖ Missing values after filling:\n",
      "üéâ All missing values filled successfully!\n",
      "\n",
      "üíæ Filled dataset saved to: gold_based_merged_filled_20250703_113946.csv\n",
      "Full path: D:\\College Life\\Semester 4\\Big Data & Predictive Analysis\\Final Project\\Testing\\gold_based_merged_filled_20250703_113946.csv\n",
      "\n",
      "üìä Dataset Summary:\n",
      "- Original missing values: 11160\n",
      "- After filling missing values: 0\n",
      "- Improvement: 11160 values filled\n",
      "- Dataset shape: (1899, 21)\n",
      "\n",
      "Sample data (first 10 rows):\n",
      "     tanggal  WTI_Price_USD  inflation_rate  close_rate  close_index\n",
      "0 2020-01-01          61.17        2.680000        1.55      6283.58\n",
      "1 2020-01-02          61.17        2.689677        1.55      6283.58\n",
      "2 2020-01-03          63.00        2.699355        1.55      6323.47\n",
      "3 2020-01-04          63.00        2.709032        1.55      6323.47\n",
      "4 2020-01-05          63.00        2.718710        1.55      6323.47\n",
      "5 2020-01-06          63.27        2.728387        1.55      6257.40\n",
      "6 2020-01-07          62.70        2.738065        1.55      6279.35\n",
      "7 2020-01-08          59.65        2.747742        1.55      6225.69\n",
      "8 2020-01-09          59.56        2.757419        1.55      6274.49\n",
      "9 2020-01-10          59.02        2.767097        1.54      6274.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16612\\1832302996.py:39: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_filled[col] = df_filled[col].fillna(method='ffill')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16612\\1832302996.py:41: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_filled[col] = df_filled[col].fillna(method='bfill')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16612\\1832302996.py:48: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_filled['inflation_rate'] = df_filled['inflation_rate'].fillna(method='ffill')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16612\\1832302996.py:49: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_filled['inflation_rate'] = df_filled['inflation_rate'].fillna(method='bfill')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16612\\1832302996.py:57: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_filled[col] = df_filled[col].fillna(method='ffill')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_16612\\1832302996.py:58: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_filled[col] = df_filled[col].fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# Fill Missing Values pada Dataset Final\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Load dataset yang sudah di-merge\n",
    "data_path = r\"D:\\College Life\\Semester 4\\Big Data & Predictive Analysis\\Final Project\\Testing\"\n",
    "gold_based_files = [f for f in os.listdir(data_path) if f.startswith('gold_based_merged_dataset_')]\n",
    "latest_file = sorted(gold_based_files)[-1]\n",
    "file_path = os.path.join(data_path, latest_file)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(file_path)\n",
    "df['tanggal'] = pd.to_datetime(df['tanggal'])\n",
    "\n",
    "print(f\"üìä Original dataset: {df.shape}\")\n",
    "print(f\"Missing values before filling:\")\n",
    "missing_before = df.isnull().sum()\n",
    "for col, count in missing_before.items():\n",
    "    if count > 0:\n",
    "        print(f\"- {col}: {count} missing ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Sort by date untuk memastikan urutan yang benar\n",
    "df = df.sort_values('tanggal').reset_index(drop=True)\n",
    "\n",
    "# Buat copy untuk filling\n",
    "df_filled = df.copy()\n",
    "\n",
    "# 1. Forward Fill untuk data finansial (rates, prices, indices)\n",
    "financial_columns = [\n",
    "    'WTI_Price_USD', 'close_rate', 'open_rate', 'high_rate', 'low_rate',\n",
    "    'close_index', 'open_index', 'high_index', 'low_index'\n",
    "]\n",
    "\n",
    "print(f\"\\nüîÑ Filling financial data with forward fill...\")\n",
    "for col in financial_columns:\n",
    "    if col in df_filled.columns:\n",
    "        # Forward fill terlebih dahulu\n",
    "        df_filled[col] = df_filled[col].fillna(method='ffill')\n",
    "        # Jika masih ada missing di awal, gunakan backward fill\n",
    "        df_filled[col] = df_filled[col].fillna(method='bfill')\n",
    "\n",
    "# 2. Interpolasi untuk data inflasi (data bulanan)\n",
    "if 'inflation_rate' in df_filled.columns:\n",
    "    print(\"üìà Interpolating inflation rate...\")\n",
    "    df_filled['inflation_rate'] = df_filled['inflation_rate'].interpolate(method='linear')\n",
    "    # Fill remaining NaN with forward/backward fill\n",
    "    df_filled['inflation_rate'] = df_filled['inflation_rate'].fillna(method='ffill')\n",
    "    df_filled['inflation_rate'] = df_filled['inflation_rate'].fillna(method='bfill')\n",
    "\n",
    "# 3. Fill change_percent dengan interpolasi\n",
    "change_columns = ['change_percent_x', 'change_percent_y']\n",
    "for col in change_columns:\n",
    "    if col in df_filled.columns:\n",
    "        print(f\"üìä Interpolating {col}...\")\n",
    "        df_filled[col] = df_filled[col].interpolate(method='linear')\n",
    "        df_filled[col] = df_filled[col].fillna(method='ffill')\n",
    "        df_filled[col] = df_filled[col].fillna(method='bfill')\n",
    "\n",
    "# 4. Fill volume dengan 0 atau interpolasi\n",
    "volume_columns = ['volume_billion_x', 'volume_billion_y']\n",
    "for col in volume_columns:\n",
    "    if col in df_filled.columns:\n",
    "        if df_filled[col].isnull().all():  # Jika semua NaN\n",
    "            print(f\"üî¢ Filling {col} with 0 (all missing)...\")\n",
    "            df_filled[col] = 0\n",
    "        else:\n",
    "            print(f\"üìä Interpolating {col}...\")\n",
    "            df_filled[col] = df_filled[col].interpolate(method='linear')\n",
    "            df_filled[col] = df_filled[col].fillna(0)  # Fill remaining with 0\n",
    "\n",
    "print(f\"\\n‚úÖ Missing values after filling:\")\n",
    "missing_after = df_filled.isnull().sum()\n",
    "total_missing_after = missing_after.sum()\n",
    "\n",
    "if total_missing_after == 0:\n",
    "    print(\"üéâ All missing values filled successfully!\")\n",
    "else:\n",
    "    print(f\"Remaining missing values: {total_missing_after}\")\n",
    "    for col, count in missing_after.items():\n",
    "        if count > 0:\n",
    "            print(f\"- {col}: {count} missing\")\n",
    "\n",
    "# Save filled dataset\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f\"gold_based_merged_filled_{timestamp}.csv\"\n",
    "output_path = os.path.join(data_path, output_filename)\n",
    "\n",
    "df_filled.to_csv(output_path, index=False)\n",
    "print(f\"\\nüíæ Filled dataset saved to: {output_filename}\")\n",
    "print(f\"Full path: {output_path}\")\n",
    "\n",
    "# Display comparison\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"- Original missing values: {missing_before.sum()}\")\n",
    "print(f\"- After filling missing values: {missing_after.sum()}\")\n",
    "print(f\"- Improvement: {missing_before.sum() - missing_after.sum()} values filled\")\n",
    "print(f\"- Dataset shape: {df_filled.shape}\")\n",
    "\n",
    "# Show sample of filled data\n",
    "print(f\"\\nSample data (first 10 rows):\")\n",
    "print(df_filled[['tanggal', 'WTI_Price_USD', 'inflation_rate', 'close_rate', 'close_index']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bb8751c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Contoh Federal Funds Rate filling:\n",
      "Sebelum dan sesudah filling missing values\n",
      "\n",
      "Data Federal Funds Rate di sekitar 2025-03-10 sampai 2025-03-18:\n",
      "======================================================================\n",
      "2025-03-08: close=4.33, open=4.33, high=4.33, low=4.33\n",
      "2025-03-09: close=4.33, open=4.33, high=4.33, low=4.33\n",
      "2025-03-10: close=4.33, open=4.33, high=4.33, low=4.33\n",
      "2025-03-11: close=4.33, open=4.33, high=4.33, low=4.33\n",
      "2025-03-12: close=4.33, open=4.33, high=4.33, low=4.33\n",
      "2025-03-13: close=4.33, open=4.33, high=4.33, low=4.33\n",
      "\n",
      "üìà Metode yang digunakan:\n",
      "1. Forward Fill: Nilai kosong diisi dengan nilai terakhir yang ada\n",
      "2. Contoh: Jika tanggal 10 = 4.33 dan tanggal 12 = 4.33\n",
      "   Maka tanggal 11 akan diisi = 4.33 (forward fill dari tanggal 10)\n",
      "3. Jika tanggal 13-17 kosong, akan diisi dengan nilai tanggal 12\n",
      "   sampai ada nilai baru di tanggal 18\n",
      "\n",
      "‚úÖ Verification - Missing values dalam dataset:\n",
      "üéâ SEMUA missing values sudah terisi!\n",
      "\n",
      "Dataset shape: (1899, 21)\n"
     ]
    }
   ],
   "source": [
    "# Contoh Spesifik: Fill Federal Funds Rate\n",
    "# Menunjukkan bagaimana data federal rate diisi berdasarkan range tanggal\n",
    "\n",
    "# Load dataset yang sudah diisi\n",
    "data_path = r\"D:\\College Life\\Semester 4\\Big Data & Predictive Analysis\\Final Project\\Testing\"\n",
    "filled_files = [f for f in os.listdir(data_path) if f.startswith('gold_based_merged_filled_')]\n",
    "\n",
    "if filled_files:\n",
    "    latest_filled = sorted(filled_files)[-1]\n",
    "    filled_path = os.path.join(data_path, latest_filled)\n",
    "    df_filled = pd.read_csv(filled_path)\n",
    "    df_filled['tanggal'] = pd.to_datetime(df_filled['tanggal'])\n",
    "    \n",
    "    # Contoh: Lihat data federal rate di sekitar tanggal 2025-03-10 sampai 2025-03-18\n",
    "    print(\"üîç Contoh Federal Funds Rate filling:\")\n",
    "    print(\"Sebelum dan sesudah filling missing values\\n\")\n",
    "    \n",
    "    # Filter data untuk contoh tanggal\n",
    "    example_dates = df_filled[\n",
    "        (df_filled['tanggal'] >= '2025-03-08') & \n",
    "        (df_filled['tanggal'] <= '2025-03-20')\n",
    "    ].copy()\n",
    "    \n",
    "    if not example_dates.empty:\n",
    "        print(\"Data Federal Funds Rate di sekitar 2025-03-10 sampai 2025-03-18:\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        federal_cols = ['tanggal', 'close_rate', 'open_rate', 'high_rate', 'low_rate']\n",
    "        display_cols = [col for col in federal_cols if col in example_dates.columns]\n",
    "        \n",
    "        for _, row in example_dates[display_cols].iterrows():\n",
    "            date_str = row['tanggal'].strftime('%Y-%m-%d')\n",
    "            if 'close_rate' in row:\n",
    "                print(f\"{date_str}: close={row['close_rate']:.2f}, open={row['open_rate']:.2f}, high={row['high_rate']:.2f}, low={row['low_rate']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìà Metode yang digunakan:\")\n",
    "    print(\"1. Forward Fill: Nilai kosong diisi dengan nilai terakhir yang ada\")\n",
    "    print(\"2. Contoh: Jika tanggal 10 = 4.33 dan tanggal 12 = 4.33\")\n",
    "    print(\"   Maka tanggal 11 akan diisi = 4.33 (forward fill dari tanggal 10)\")\n",
    "    print(\"3. Jika tanggal 13-17 kosong, akan diisi dengan nilai tanggal 12\")\n",
    "    print(\"   sampai ada nilai baru di tanggal 18\")\n",
    "    \n",
    "    # Show verification\n",
    "    print(f\"\\n‚úÖ Verification - Missing values dalam dataset:\")\n",
    "    missing_check = df_filled.isnull().sum()\n",
    "    total_missing = missing_check.sum()\n",
    "    \n",
    "    if total_missing == 0:\n",
    "        print(\"üéâ SEMUA missing values sudah terisi!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Masih ada {total_missing} missing values:\")\n",
    "        for col, count in missing_check.items():\n",
    "            if count > 0:\n",
    "                print(f\"- {col}: {count}\")\n",
    "    \n",
    "    print(f\"\\nDataset shape: {df_filled.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Filled dataset tidak ditemukan. Jalankan cell sebelumnya dulu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d772ad8e",
   "metadata": {},
   "source": [
    "Memberikan missing values jadi terisi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a69d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
